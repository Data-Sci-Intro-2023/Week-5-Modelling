---
title: "Tidying Public Water Quality Data"
author: "Matthew Ross"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc:  yes
    toc_depth:  3
    toc_float: true
editor_options: 
  chunk_output_type: console
---



# Why public datasets?

Working with large, open-access datasets can serve many purposes. It can be an excellent way to explore new ideas, before investing in field-work or experiments. It can be a great way to take local or experimental results and expand them to different ecosystems, places, or landscapes. Or it can be an excellent way to build, validate, and test ecological models on regional or national scales. 

So why doesn't everyone use public data? Well, it's often collected by a variety of organizations, with different methods, units, and incosistent metadata. Together these issues with large public datasets, make them "messy." Messy data can be messy in many different ways, but at the basic level it means that data is hard to analyze, not because the data itself is bad, but because the way it is organized is unclear or inconsistent. 

In this lab, we will learn some tricks to "tidying" data, making it analysis-ready. We will depend heavily on the [tidyverse](https://www.tidyverse.org/), an excellent series of packages that make data manipulation beautiful and easy. We will also be working with water quality portal data so we will also use the excellent [dataRetrieval](https://github.com/USGS-R/dataRetrieval) package for downloading data from the Water Quality Portal and the USGS. 

## Loading key packages

This lab is meant to introduce the incredible variety of tools that one can use to clean data, many of these tools are captured by the `tidyverse` meta-package, a package of packages, but there are some additional ones that will help us locate our various water quality sites. 


```{r setup, warnings='hide',message=FALSE}
library(tidyverse) # Package with dplyr, tibble, readr, and others to help clean coding
library(dataRetrieval) # Package to download data. 
library(sf) #Geospatial package to plot and explore data
library(mapview) #Simple interface to leaflet interactive maps
library(broom) #Simplifies model outputs
library(knitr) #Makes nice tables
library(kableExtra) #Makes even nicer tables
library(lubridate) #Makes working with dates easier
library(ggthemes) #Makes plots prettier
library(tidyr) #Makes multiple simultaneous models easier


#Move the directory to the top folder level
knitr::opts_knit$set(root.dir='..')

```

# Downloading data.

For this lab, we'll explore water quality data in the Colorado River basin as it moves from Colorado to Arizona. All data will be generated through the code you see below, with the only external information coming from knowing the SiteID's for the monitoring locations along the Colorado River and the water quality characteristic names. 


The water quality portal can be accessed with the command `readWQPdata`, which takes a variety of parameters (like startdate, enddate, constituents, etc...). We'll generate these rules for downloading the data here. 

## Download prep

```{r download-prep}
#First we'll make a tibble (a tidyverse table) with Site IDs. Generally these are increasingly downstream of the CO headwaters near Grand Lake. 
colorado <- tibble(sites=c('USGS-09034500','USGS-09069000',
                           'USGS-09085000','USGS-09095500','USGS-09152500'),
                   basin=c('colorado1','eagle',
                           'roaring','colorado3','gunnison'))

#Now we need to setup a series of rules for downloading data from the Water Quality Portal. 
#We'll focus on cation and anion data from 1950-present. Each cation has a name that we might typically use like calcium or sulfate, but the name may be different in the water quality portal, so we have to check this website https://www.waterqualitydata.us/Codes/Characteristicname?mimeType=xml to get our names correct. 

paramater.names <- c('ca','mg','na','k','so4','cl','hco3')

ca <- c('Calcium')
mg <- c('Magnesium')
na <- 'Sodium'
k <- 'Potassium'
so4 <- c('Sulfate','Sulfate as SO4','Sulfur Sulfate','Total Sulfate')
cl <- 'Chloride'
hco3 <- c('Alkalinity, bicarbonate','Bicarbonate')

#Compile all these names into a single list
parameters <- list(ca,mg,na,k,so4,cl,hco3)
#Name each cation or anion in the list
names(parameters) <- paramater.names
#Notice that we aren't downloading any nutrients (P or N) because they are much messier (100s of different ways to measure and report concentration data) than other cation anion data. 

#Start dates
start <- '1980-10-01'
end <- '2023-01-01'

#Sample media (no sediment samples)
sampleMedia = 'Water'

#Comple all this information into a list with arguments
site.args <- list(siteid=colorado$sites,
                  sampleMedia=sampleMedia,
                  startDateLo=start,
                  startDateHi=end,
                  characteristicName=NA) #We'll fill this in later in a loop



```

## Concentration data download

Now that we have generated the commands to download the data, the code to download the data is here, but it is not run on purpose because it takes 15 minutes or so to run everytime. You can always run it yourself by setting `eval=T`. 


```{r concentration download}
conc.list <- list() #Empty list to hold each data download


#We'll loop over each anion or cation and download all data at our sites for that constituent
for(i in 1:length(parameters)){
  #We need to rename the characteristicName (constituent) each time we go through the loop
  site.args$characteristicName<-parameters[[i]]
  
  #readWQPdata takes in our site.args list and downloads the data according to those rules 
  # time, constituent, site, etc...
  
  # Don't forget about pipes "%>%"! Pipes pass forward the results of a previous command, so that 
  #You don't have to constantly rename variables. I love them. 
  
  conc.list[[i]] <- readWQPdata(site.args) %>%
    mutate(parameter=names(parameters)[i]) #Mutate just adds a new column to the data frame
  
  #Pipes make the above command simple and succinct versus something more complicated like:
  
  ## conc.list[[i]] <- readWQPdata(site.args) 
  ## conc.list[[i]]$parameter <- names(parameters)[i]
}

#bind all this data together into a single data frame
conc.long <- map_dfr(conc.list,rbind)



```




# Data tidying

Now that we have downloaded the data we need to tidy it up. The water quality portal data comes with an incredible amount of metadata in the form of extra columns. But we don't need all this extra data. 


## Look at the data you downloaded.



```{r conc data}
head(conc.long) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='800px',height='300px')

```


## Initial cleaning up

Wow that looks messy! Lots of extraneous columns, lots of NAs, so much information we can hardly parse it. Let's pair it down to the essentials. 

```{r tidying up concentration}
#This code mostly just grabs and renames the most important data columns
conc.clean <-  conc.long %>%
  dplyr::select(date=ActivityStartDate,
                parameter=CharacteristicName,
                units=ResultMeasure.MeasureUnitCode,
                SiteID=MonitoringLocationIdentifier,
                org=OrganizationFormalName,
                org_id=OrganizationIdentifier,
                time=ActivityStartTime.Time,
                value=ResultMeasureValue,
                sample_method=SampleCollectionMethod.MethodName,
                analytical_method=ResultAnalyticalMethod.MethodName,
                particle_size=ResultParticleSizeBasisText,
                date_time=ActivityStartDateTime,
                media=ActivityMediaName,
                sample_depth=ActivityDepthHeightMeasure.MeasureValue,
                sample_depth_unit=ActivityDepthHeightMeasure.MeasureUnitCode,
                fraction=ResultSampleFractionText,
                status=ResultStatusIdentifier) %>%
  #Remove trailing white space in labels
  mutate(units = trimws(units)) %>%
  #Keep only samples that are water samples
  filter(media=='Water') #Some of these snuck through!

```

Now let's look at the tidier version 
```{r examine tidier data}
head(conc.clean) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='800px',height='300px')
```

## Final tidy dataset

Okay that is getting better but we still have lots of extraneous information. For our purposes let's assume that the sample and analytical methods used by the USGS are reasonable and exchangeable (one method is equivalent to the other). If we make that assumption then the only remaining data we need to clean is to make sure that all the data has the same units. 

### Unit Check

```{r unit check}
table(conc.clean$units)
```

Wow! Almost all the data is in mg/L. That makes our job really easy. 

We just need to remove these observations with a `dplyr::filter` call and then select an even smaller subset of useful columns, while adding a time object column using the `lubridate::ymd` call. 

```{r tidy}
conc.tidy <- conc.clean %>% 
  filter(units == 'mg/l') %>%
  mutate(date=ymd(date)) %>%
  select(date,
         parameter,
         SiteID,
         conc=value)


```

### Daily data

Okay now we have a manageable data frame. But how do we want to organize the data? Since we are looking at a really long time-series of data (70 years), let's look at data as a daily average. The `dplyr::group_by and summarize` commands make this really easy

```{r daily}


#The amazing group_by function groups all the data so that the summary
#only applies to each subgroup (site, date, and parameter combination).
#So in the end you get a daily average concentratino for each site and parameter type. 
conc.daily <- conc.tidy %>%
  group_by(date,parameter,SiteID) %>% 
  summarize(conc=mean(conc,na.rm=T))

```

Taking daily averages looks like it did eliminate `r nrow(conc.tidy) - nrow(conc.daily)` observations, meaning these site date combinations had multiple observations on the same day. 





# Assignment!

Let's imagine you wanted to add data for your water quality analyses, 
but you also know that you need to do this analysis over and over again. Let's
walk through how we would: 1) Add new data to our `conc.clean` dataset and 2)
how to write a function to download, clean, and update our data with far less code.

## Question 1.


Write a function that can repeat the above steps with a single function call. This
function should take in a single tibble that is identical in structure to the 
`colorado` one above (e.g. it has columns named `sites`, and `basin`). The function should
then take in that tibble and be able to download and clean data for the any of
the sites we listed above to make the data look exactly like `conc.daily`. Use this function to download data for the three sites listed below. 

```{r}


additional_data <- tibble(sites = c('USGS-09180000','USGS-09180500','USGS-09380000'),
                          basin = c('dolores','colorado4','colorado5'))

```


## Question 2

Using the function above, download and clean water quality data for the sites 
and basin names listed below. This data should look identical to your `conc.daily`
dataset. The steps are:


1. In the function -  Download the new data

2. In the function - Clean the new data and make it look identical to `conc.tidy`

3. With the data the function returns - Append the new data to `conc.daily` using `bind_rows()`

4. Save the new data as `tidied_full_wq.RData`




```{r}


#save(wq, file = 'data/tidied_full_wq.RData')
```




## Question 3

We now have a dataset of stream water quality data for 9 sites throughout Colorado.
One potential control on stream chemistry is stream discharge. One function that 
can allow you to easily download discharge data is `readNWISdv()` from the 
dataRetrieval package. Use this function to download daily discharge data for 
all eight of the sites listed above (including the additional data). Save this
data as `data/Q.RData`. The site numbers are the same as above but you need 
to remove `USGS-` from each site. Reminder, discharge is `00060` parameterCd and
you can use `renameNWISColumns()` to automatically make the column names a 
little less annoying. 

```{r}
# Reminder! you can use ?readNWISdv to read about how the function works. 
sites <- colorado %>%
  #Bind the two datasets to get all 8 sites
  bind_rows(additional_data) %>%
  #Grab just the column labeled sites
  pull(sites) %>%
  #Remove the USGS- prefix
  gsub('USGS-','',.)



#save(q_data, file = 'data/Q.Rdata')
```



